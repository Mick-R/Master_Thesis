{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80a872d-d3f7-4aef-843a-66443b722b17",
   "metadata": {},
   "source": [
    "# Inter-rater Reliability Between Human Experts\n",
    "\n",
    "## Framework Validation: Pilot Study\n",
    "\n",
    "This notebook presents the pilot validation of empirically derived quality metrics for job listing evaluation through human expert inter-rater reliability (IRR) analysis. The analysis validates whether recruitment experts can consistently apply the eight quality metrics across four dimensions (Clarity, Relevance, Correctness, Completeness) derived from requirements elicitation.\n",
    "\n",
    "### Research Context\n",
    "- **Objective**: Validate metric interpretability among recruitment experts\n",
    "- **Sample**: Job listings rated by human experts (Dutch and English)\n",
    "- **Methodology**: Gwet's $AC_1$ coefficient for reliability assessment\n",
    "- **Significance**: Establishes foundation for LLM-based evaluation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0af6e986-5ccd-4017-b12f-bbd9472e76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pwr)\n",
    "library(irr)\n",
    "library(kappaSize)\n",
    "library(readxl)\n",
    "library(dplyr)\n",
    "library(irrCAC)\n",
    "library(knitr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb4b60-75a1-435f-90d1-cb0bfb2f5522",
   "metadata": {},
   "source": [
    "## 1. Sample Size Determination\n",
    "\n",
    "**Statistical Parameters for Pilot Study:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Gwet's $AC_1$ Interpretation Framework:\n",
    "| $AC_1$ Range | Interpretation          |\n",
    "|---------------|-------------------------|\n",
    "| < 0           | Poor agreement          |\n",
    "| 0.00–0.20     | Slight agreement        |\n",
    "| 0.20–0.40     | Fair agreement          |\n",
    "| 0.40–0.60     | Moderate agreement      |\n",
    "| 0.60–0.80     | Substantial agreement   |\n",
    "| 0.80–1.00     | Almost perfect agreement|\n",
    "\n",
    "Gwet's $AC_1$ is preferred over Cohen's kappa due to its stability with extreme prevalence distributions, which is relevant for job listing quality ratings where most listings may receive positive evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0846a48-b733-4170-9eec-9283b1febcc6",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "### Survey Data\n",
    "Loading expert evaluation data from Qualtrics surveys containing ratings on the eight empirically derived quality metrics. Expert raters evaluated job listings using 5-point Likert scales across both Dutch and English samples.\n",
    "\n",
    "**Data Structure:**\n",
    "- Dutch sample: Recruitment experts rating Dutch job listings\n",
    "- English sample: Recruitment experts rating English job listings\n",
    "- Variables: Expert ratings on 8 quality dimensions\n",
    "- Rating scale: 1 (Strongly Disagree) to 5 (Strongly Agree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "27adf8f4-7051-4760-80e9-db08bcc55ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch <- read_excel(\"Dutch_surveys_ICR_2.xlsx\")\n",
    "df_english <- read_excel(\"English_surveys_ICR_2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ad3d7-9be6-4747-9f1f-8c36b62de045",
   "metadata": {},
   "source": [
    "### Data Preparation and Standardization\n",
    "\n",
    "**Variable Standardization:**\n",
    "Renaming Qualtrics-generated response IDs to standardized rater identifiers for analysis consistency. The numeric conversion ensures proper statistical computation for reliability coefficients.\n",
    "\n",
    "**Binary Classification Function:**\n",
    "The `binarize()` function implements the threshold confirmed by all participating recruitment companies: ratings ≥3 represent \"acceptable\" quality, while ratings <3 represent \"unacceptable\" quality. This binary classification enables clearer interpretation of agreement patterns and aligns with practical decision-making in recruitment contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "53c3d722-d3a2-4d21-8d05-a3ea5c2a546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch <- df_dutch %>%\n",
    "  rename(\n",
    "    rater1 = R_8q7mN6gv7GC7sxd,\n",
    "    rater2 = R_2wT7oFicvVDdz9L\n",
    "  )\n",
    "\n",
    "df_english <- df_english %>%\n",
    "  rename(\n",
    "    rater1 = R_2F4IWrRogs2MEvL,\n",
    "    rater2 = R_2rkb0tBGzURC0Ia\n",
    "  )\n",
    "\n",
    "df_dutch <- df_dutch %>%\n",
    "  mutate(\n",
    "    rater1 = as.numeric(rater1),\n",
    "    rater2 = as.numeric(rater2)\n",
    "  )\n",
    "\n",
    "df_english <- df_english %>%\n",
    "  mutate(\n",
    "    rater1 = as.numeric(rater1),\n",
    "    rater2 = as.numeric(rater2)\n",
    "  )\n",
    "\n",
    "binarize <- function(x) ifelse(x >= 3, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6701ab-392b-46e0-9439-9b1ab527f312",
   "metadata": {},
   "source": [
    "## 3. Descriptive Analysis by Response Category\n",
    "\n",
    "### Individual Job Listing Analysis\n",
    "Computing descriptive statistics for each job listing (ResponseId) stratified by generation method (Generated: 0=Human-written, 1=LLM-generated). This analysis provides insight into rating patterns across different job listings and generation methods.\n",
    "\n",
    "**Metrics Calculated:**\n",
    "- Central tendency: Mean, median\n",
    "- Variability: Standard deviation, IQR\n",
    "- Range: Minimum, maximum values\n",
    "\n",
    "These descriptive statistics inform the subsequent reliability analysis by revealing potential systematic differences in rating patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "5c870df7-c9d6-4c8a-a898-f00909169697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 16 × 14\u001b[39m\n",
      "   Generated ResponseId mean_rater1 mean_rater2 sd_rater1 sd_rater2\n",
      "       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m         0          1         3.5         3.6     0.707     1.51 \n",
      "\u001b[90m 2\u001b[39m         0          2         2.9         3.4     0.316     0.966\n",
      "\u001b[90m 3\u001b[39m         0          3         3.7         2.7     0.823     1.57 \n",
      "\u001b[90m 4\u001b[39m         0          4         3.2         2.7     1.23      1.57 \n",
      "\u001b[90m 5\u001b[39m         0          5         2.6         3.4     0.516     0.966\n",
      "\u001b[90m 6\u001b[39m         0          6         3.7         3.3     0.823     1.42 \n",
      "\u001b[90m 7\u001b[39m         0          7         3.7         2.8     0.675     1.23 \n",
      "\u001b[90m 8\u001b[39m         0          8         3.1         2.9     1.45      1.60 \n",
      "\u001b[90m 9\u001b[39m         1          1         4.2         3.8     0.632     0.632\n",
      "\u001b[90m10\u001b[39m         1          2         3.5         3.7     0.850     0.675\n",
      "\u001b[90m11\u001b[39m         1          3         4.3         3.1     0.675     1.37 \n",
      "\u001b[90m12\u001b[39m         1          4         3.7         3.1     0.823     1.37 \n",
      "\u001b[90m13\u001b[39m         1          5         2.8         3.2     0.632     1.03 \n",
      "\u001b[90m14\u001b[39m         1          6         3.8         3.1     1.03      0.994\n",
      "\u001b[90m15\u001b[39m         1          7         4.1         3.8     0.738     0.919\n",
      "\u001b[90m16\u001b[39m         1          8         4.7         4.5     0.675     0.707\n",
      "\u001b[90m# ℹ 8 more variables: median_rater1 <dbl>, median_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   min_rater1 <dbl>, min_rater2 <dbl>, max_rater1 <dbl>, max_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   iqr_rater1 <dbl>, iqr_rater2 <dbl>\u001b[39m\n",
      "\u001b[90m# A tibble: 16 × 14\u001b[39m\n",
      "   Generated ResponseId mean_rater1 mean_rater2 sd_rater1 sd_rater2\n",
      "       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m         0          1         3.9         3.6     0.994     1.26 \n",
      "\u001b[90m 2\u001b[39m         0          2         3.1         3.4     1.66      0.843\n",
      "\u001b[90m 3\u001b[39m         0          3         3.1         2.9     1.29      0.738\n",
      "\u001b[90m 4\u001b[39m         0          4         2.4         2.4     1.35      0.516\n",
      "\u001b[90m 5\u001b[39m         0          5         3.1         3.3     1.20      0.823\n",
      "\u001b[90m 6\u001b[39m         0          6         3.4         3.6     1.26      1.17 \n",
      "\u001b[90m 7\u001b[39m         0          7         3.8         2.3     1.40      1.16 \n",
      "\u001b[90m 8\u001b[39m         0          8         1.8         2       0.632     1.33 \n",
      "\u001b[90m 9\u001b[39m         1          1         4.6         3.7     0.516     1.16 \n",
      "\u001b[90m10\u001b[39m         1          2         3.3         3.5     1.25      0.850\n",
      "\u001b[90m11\u001b[39m         1          3         4.3         3.5     0.675     1.08 \n",
      "\u001b[90m12\u001b[39m         1          4         3.4         3.2     0.516     1.14 \n",
      "\u001b[90m13\u001b[39m         1          5         4.4         3.8     1.07      0.632\n",
      "\u001b[90m14\u001b[39m         1          6         3.8         3.7     1.23      1.16 \n",
      "\u001b[90m15\u001b[39m         1          7         4.3         3       1.06      1.63 \n",
      "\u001b[90m16\u001b[39m         1          8         4.3         4.2     0.823     1.48 \n",
      "\u001b[90m# ℹ 8 more variables: median_rater1 <dbl>, median_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   min_rater1 <dbl>, min_rater2 <dbl>, max_rater1 <dbl>, max_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   iqr_rater1 <dbl>, iqr_rater2 <dbl>\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics <- function(data) {\n",
    "  data %>%\n",
    "    group_by(Generated, ResponseId) %>%\n",
    "    summarise(\n",
    "      mean_rater1 = mean(rater1, na.rm = TRUE),\n",
    "      mean_rater2 = mean(rater2, na.rm = TRUE),\n",
    "      sd_rater1 = sd(rater1, na.rm = TRUE),\n",
    "      sd_rater2 = sd(rater2, na.rm = TRUE),\n",
    "      median_rater1 = median(rater1, na.rm = TRUE),\n",
    "      median_rater2 = median(rater2, na.rm = TRUE),\n",
    "      min_rater1 = min(rater1, na.rm = TRUE),\n",
    "      min_rater2 = min(rater2, na.rm = TRUE),\n",
    "      max_rater1 = max(rater1, na.rm = TRUE),\n",
    "      max_rater2 = max(rater2, na.rm = TRUE),\n",
    "      iqr_rater1 = IQR(rater1, na.rm = TRUE),\n",
    "      iqr_rater2 = IQR(rater2, na.rm = TRUE),\n",
    "      .groups = \"drop\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "eval_dutch <- evaluation_metrics(df_dutch)\n",
    "eval_english <- evaluation_metrics(df_english)\n",
    "\n",
    "print(eval_dutch)\n",
    "print(eval_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5237461-5cfc-457d-bea9-26768e81b477",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis: Human vs. LLM-Generated Content\n",
    "\n",
    "### Generation Method Comparison\n",
    "Aggregating ratings by generation method to examine systematic differences between human-written and LLM-generated job listings. This analysis addresses the sub-research question regarding quality differences between generation methods.\n",
    "\n",
    "**Key Comparisons:**\n",
    "- Mean rating differences between generation methods\n",
    "- Variability patterns across human vs. LLM content\n",
    "- Consistency of expert evaluations by generation type\n",
    "\n",
    "The results inform both the validation of the generation framework and the reliability of expert evaluations across different content sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "717e2e7a-d261-4b87-a905-022bd3cfe4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 2 × 13\u001b[39m\n",
      "  Generated mean_rater1 mean_rater2 sd_rater1 sd_rater2 median_rater1\n",
      "      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m         0        3.3         3.1      0.933      1.36             3\n",
      "\u001b[90m2\u001b[39m         1        3.89        3.54     0.914      1.07             4\n",
      "\u001b[90m# ℹ 7 more variables: median_rater2 <dbl>, iqr_rater1 <dbl>, iqr_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   min_rater1 <dbl>, min_rater2 <dbl>, max_rater1 <dbl>, max_rater2 <dbl>\u001b[39m\n",
      "\u001b[90m# A tibble: 2 × 13\u001b[39m\n",
      "  Generated mean_rater1 mean_rater2 sd_rater1 sd_rater2 median_rater1\n",
      "      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m         0        3.08        2.94      1.37      1.14             3\n",
      "\u001b[90m2\u001b[39m         1        4.05        3.58      1.01      1.18             4\n",
      "\u001b[90m# ℹ 7 more variables: median_rater2 <dbl>, iqr_rater1 <dbl>, iqr_rater2 <dbl>,\u001b[39m\n",
      "\u001b[90m#   min_rater1 <dbl>, min_rater2 <dbl>, max_rater1 <dbl>, max_rater2 <dbl>\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "summary_generated_vs_human <- function(df) {\n",
    "  df %>%\n",
    "    mutate(\n",
    "      rater1 = as.numeric(rater1),\n",
    "      rater2 = as.numeric(rater2)\n",
    "    ) %>%\n",
    "    group_by(Generated) %>%\n",
    "    summarise(\n",
    "      mean_rater1 = mean(rater1, na.rm = TRUE),\n",
    "      mean_rater2 = mean(rater2, na.rm = TRUE),\n",
    "      sd_rater1   = sd(rater1, na.rm = TRUE),\n",
    "      sd_rater2   = sd(rater2, na.rm = TRUE),\n",
    "      median_rater1 = median(rater1, na.rm = TRUE),\n",
    "      median_rater2 = median(rater2, na.rm = TRUE),\n",
    "      iqr_rater1 = IQR(rater1, na.rm = TRUE),\n",
    "      iqr_rater2 = IQR(rater2, na.rm = TRUE),\n",
    "      min_rater1 = min(rater1, na.rm = TRUE),\n",
    "      min_rater2 = min(rater2, na.rm = TRUE),\n",
    "      max_rater1 = max(rater1, na.rm = TRUE),\n",
    "      max_rater2 = max(rater2, na.rm = TRUE),\n",
    "      .groups = \"drop\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run for both datasets\n",
    "summary_dutch   <- summary_generated_vs_human(df_dutch)\n",
    "summary_english <- summary_generated_vs_human(df_english)\n",
    "\n",
    "print(summary_dutch)\n",
    "print(summary_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fde910-75ad-403d-a0a2-11671426dcb4",
   "metadata": {},
   "source": [
    "## 5. Gwet's $AC_1$ Analysis and Sample Size Estimation\n",
    "\n",
    "### Advanced Reliability Assessment\n",
    "Gwet's $AC_1$ coefficient provides more stable reliability estimates than Cohen's kappa, particularly important for quality evaluation studies where high prevalence of positive ratings is expected.\n",
    "\n",
    "**Combined Language Analysis:**\n",
    "- Pooling Dutch and English samples for robust reliability estimates\n",
    "- Response-level analysis across all eight job listing categories\n",
    "- Sample size estimation for future large-scale validation studies\n",
    "\n",
    "**Practical Implications:**\n",
    "The $AC_1$ coefficients inform which quality metrics demonstrate sufficient inter-rater reliability for inclusion in the automated evaluation framework. Sample size estimations guide the planning of full-scale validation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "f28594a4-4fcc-4c3d-a484-6cf442b972aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINED RESULTS ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ResponseId   prevalence   gwet_ac1   sample_size \n",
       "-----------  -----------  ---------  ------------\n",
       "1            0.89         0.72       156         \n",
       "2            0.85         0.60       185         \n",
       "3            0.78         0.39       183         \n",
       "4            0.65         0.36       177         \n",
       "5            0.76         0.65       176         \n",
       "6            0.81         0.53       192         \n",
       "7            0.76         0.33       171         \n",
       "8            0.68         0.55       190         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_all_metrics <- function(df, group_var) {\n",
    "  df %>%\n",
    "    group_by(!!sym(group_var)) %>%\n",
    "    summarise(\n",
    "      prevalence = {\n",
    "        r1 <- binarize(rater1)\n",
    "        r2 <- binarize(rater2)\n",
    "        (mean(r1) + mean(r2)) / 2\n",
    "      },\n",
    "      gwet_ac1 = {\n",
    "        r1 <- binarize(rater1)\n",
    "        r2 <- binarize(rater2)\n",
    "        ratings <- data.frame(r1, r2)\n",
    "        ac1 <- gwet.ac1.raw(ratings)\n",
    "        ac1$est$coeff.val\n",
    "      },\n",
    "      sample_size = {\n",
    "        r1 <- binarize(rater1)\n",
    "        r2 <- binarize(rater2)\n",
    "        ratings <- data.frame(r1, r2)\n",
    "        ac1_val <- gwet.ac1.raw(ratings)$est$coeff.val\n",
    "        kappaSize.ac1(k0 = 0, k1 = ac1_val, alpha = 0.05, power = 0.8)$N\n",
    "      },\n",
    "      .groups = \"drop\"\n",
    "    )\n",
    "}\n",
    "\n",
    "df_dutch <- df_dutch %>% mutate(language = \"Dutch\")\n",
    "df_english <- df_english %>% mutate(language = \"English\")\n",
    "df_combined <- bind_rows(df_dutch, df_english)\n",
    "results_combined <- calculate_all_metrics(df_combined, \"ResponseId\")\n",
    "\n",
    "results_combined_display <- results_combined %>%\n",
    "  mutate(\n",
    "    prevalence = sprintf(\"%.2f\", prevalence),\n",
    "    gwet_ac1 = sprintf(\"%.2f\", gwet_ac1)\n",
    "  )\n",
    "\n",
    "cat(\"=== COMBINED RESULTS ===\\n\")\n",
    "kable(results_combined_display, format = \"simple\", align = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce97f-6ade-4f07-a9fe-b24589caa86a",
   "metadata": {},
   "source": [
    "## 6. Basic Agreement Statistics by Language and Response\n",
    "\n",
    "### Raw Agreement and Bias Analysis\n",
    "Computing fundamental agreement metrics to complement the $AC_1$ reliability coefficients. These statistics provide additional insight into the nature of disagreements between expert raters.\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Raw Agreement**: Proportion of identical ratings between raters\n",
    "- **Bias Index**: Systematic difference in rating tendencies between raters\n",
    "\n",
    "**Analytical Value:**\n",
    "Raw agreement provides an intuitive measure of rater consensus, while bias index identifies systematic rating differences that may indicate varying interpretation of quality standards or response style differences between expert raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "022ed0b2-c274-4f72-a5a0-f1822fe2ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC AGREEMENT STATISTICS BY LANGUAGE AND RESPONSE ID ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Language   Response ID   Raw Agreement   Bias Index \n",
       "---------  ------------  --------------  -----------\n",
       "Dutch      1             0.85            0.05       \n",
       "Dutch      2             0.80            0.00       \n",
       "Dutch      3             0.60            0.40       \n",
       "Dutch      4             0.80            0.20       \n",
       "Dutch      5             0.80            0.10       \n",
       "Dutch      6             0.70            0.20       \n",
       "Dutch      7             0.75            0.25       \n",
       "Dutch      8             0.85            0.05       \n",
       "English    1             0.70            0.20       \n",
       "English    2             0.60            0.20       \n",
       "English    3             0.60            0.10       \n",
       "English    4             0.50            0.20       \n",
       "English    5             0.75            0.15       \n",
       "English    6             0.65            0.05       \n",
       "English    7             0.40            0.30       \n",
       "English    8             0.65            0.05       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_basic_agreement_simplified <- function(r1, r2) {\n",
    "\n",
    "  r1_bin <- binarize(r1)\n",
    "  r2_bin <- binarize(r2)\n",
    "  \n",
    "\n",
    "  raw_agreement <- mean(r1_bin == r2_bin)\n",
    "  \n",
    "\n",
    "  prevalence_r1 <- mean(r1_bin)\n",
    "  prevalence_r2 <- mean(r2_bin)\n",
    "  bias_index <- abs(prevalence_r1 - prevalence_r2)\n",
    "  \n",
    "  return(data.frame(\n",
    "    raw_agreement = raw_agreement,\n",
    "    bias_index = bias_index\n",
    "  ))\n",
    "}\n",
    "\n",
    "basic_agreement_results <- df_combined %>%\n",
    "  group_by(language, ResponseId) %>%\n",
    "  summarise(\n",
    "    calculate_basic_agreement_simplified(rater1, rater2),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "basic_agreement_display <- basic_agreement_results %>%\n",
    "  mutate(\n",
    "    raw_agreement = sprintf(\"%.2f\", raw_agreement),\n",
    "    bias_index = sprintf(\"%.2f\", bias_index)\n",
    "  )\n",
    "\n",
    "cat(\"=== BASIC AGREEMENT STATISTICS BY LANGUAGE AND RESPONSE ID ===\\n\")\n",
    "kable(basic_agreement_display, format = \"simple\", align = \"l\",\n",
    "      col.names = c(\"Language\", \"Response ID\", \"Raw Agreement\", \"Bias Index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6941b3d-cf2d-4a1f-995b-9bf9c8ba78d7",
   "metadata": {},
   "source": [
    "## 7. Summary and Implications\n",
    "\n",
    "### Pilot Study Findings\n",
    "This analysis provides initial validation of the empirically derived quality metrics through human expert inter-rater reliability assessment. The results inform the refinement of metrics and guide the design of full-scale validation experiments.\n",
    "\n",
    "**Key Implications for Framework Development:**\n",
    "1. **Metric Reliability**: $AC_1$ coefficients indicate which quality dimensions demonstrate sufficient expert consensus\n",
    "2. **Sample Size Planning**: Estimated sample sizes guide future validation study design\n",
    "3. **Language Effects**: Systematic differences between Dutch and English evaluations suggest need for language-specific calibration\n",
    "4. **Generation Method Performance**: Preliminary evidence of LLM-generated content quality relative to human-written baselines\n",
    "\n",
    "**Next Steps:**\n",
    "These pilot results inform the development of the LLM-as-a-judge evaluation component and guide possible refinements to the quality assessment framework before large-scale implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d5772-a558-43a7-9f87-5fc731fa7d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
