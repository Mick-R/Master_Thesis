{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986287c0-30aa-47b1-8455-a8bfb1f1cf74",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Evaluation Pipeline Testing\n",
    "\n",
    "## Validation Framework: LLM-Human Agreement Analysis\n",
    "\n",
    "This notebook implements the second validation experiment from the thesis framework, focusing on **LLM-as-a-judge inter-rater reliability** testing. The analysis evaluates whether automated LLM evaluation can achieve substantial agreement with human expert judgments across the eight empirically derived quality metrics.\n",
    "\n",
    "### Research Context\n",
    "- **Objective**: Validate LLM-based evaluation reliability compared to human expert assessments\n",
    "- **Sample**: Same job listings evaluated by both human experts and LLM judges\n",
    "- **Methodology**: Dual LLM assessment (GPT-4o and Claude-3.5)\n",
    "- **Significance**: Establishes foundation for automated quality evaluation in recruitment contexts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Evaluation Framework Architecture\n",
    "\n",
    "### LLM-as-a-Judge Implementation\n",
    "The `IndividualMetricEvaluator` class implements the core evaluation framework using structured prompts that incorporate the operational definitions from Table 2 of the thesis. The system employs:\n",
    "\n",
    "**Dual Model Strategy:**\n",
    "- **GPT-4o-20240513**: Selected for demonstrated consistency with expert judgments in textual analysis\n",
    "- **Claude-3.5-sonnet-20240620**: Chosen for minimal sensitivity to evaluation biases\n",
    "- **Temperature = 0.0**: Ensures deterministic outputs for reliability assessment\n",
    "\n",
    "**Quality Dimensions Assessment:**\n",
    "- **Clarity**: Language level appropriateness and syntax/grammar adherence\n",
    "- **Relevance**: Motivating text effectiveness and creativity level appropriateness  \n",
    "- **Correctness**: Content rules compliance and tone of voice consistency\n",
    "- **Completeness**: Applicant profile reflection and company information provision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa662c0d-a386-49af-ba74-25d07f31b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Literal, Union\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3130760f-f8c9-48cb-a307-9bae45877540",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_check = pd.read_excel('english_shuffled_job_listings.xlsx')\n",
    "nl_check = pd.read_excel('dutch_shuffled_job_listings.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3659d18-c84f-4c6f-88a2-62e24c1923ac",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics (TEST)\n",
    "### 1.1 Quantifiable metrics (TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b00a10-9963-4260-84d2-ab32d6a9b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JobListingValidator:    \n",
    "    @staticmethod\n",
    "    def validate_word_count(text: str, min_words: int, max_words: int):\n",
    "        word_count = len(text.split())\n",
    "        return min_words <= word_count <= max_words\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_sentence_length(text: str, max_length: int):\n",
    "        import re\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        long_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.split()) > max_length:\n",
    "                long_sentences.append(sentence.strip())\n",
    "\n",
    "        return long_sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def check_gendered_language(text: str, language: str = \"nl\"):\n",
    "        gendered_terms = {\n",
    "            \"nl\": [\"hij\", \"zij\", \"hem\", \"haar\", \"zijn\", \"de hare\", \"man\", \"vrouw\", \"kerel\", \n",
    "                   \"meid\", \"verkoper\", \"verkoopster\", \"zakenman\", \"zakenvrouw\", \"politieman\", \n",
    "                   \"politievrouw\"],\n",
    "            \"en\": [\n",
    "                'he', 'she', 'him', 'her', 'his', 'hers', 'man', 'woman',\n",
    "                'guy', 'gal', 'salesman', 'saleswoman', 'businessman', 'businesswoman',\n",
    "                'policeman', 'policewoman', 'spokesman', 'spokeswoman']}\n",
    "    \n",
    "        terms = gendered_terms.get(language.lower(), [])\n",
    "        found_terms = []\n",
    "        text_lower = text.lower()\n",
    "    \n",
    "        for term in terms:\n",
    "            if term in text_lower:\n",
    "                found_terms.append(term)\n",
    "    \n",
    "        return found_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c39fbc-ad77-4ec1-a7d0-e868e822093b",
   "metadata": {},
   "source": [
    "## 2. Experimental Design\n",
    "\n",
    "### Sample Composition\n",
    "Following the thesis methodology (Section 6.2):\n",
    "- **Total listings**: 40 job listings (20 Dutch, 20 English)\n",
    "- **Generation split**: 50% human-written, 50% LLM-generated per language\n",
    "- **Level distribution**: 5 entry-level, 5 mid/senior-level positions per generation type\n",
    "- **Company representation**: Companies A, C, D (Dutch); Companies B, E (English)\n",
    "\n",
    "### Bias Mitigation Strategy\n",
    "The framework addresses **self-enhancement bias** concerns through:\n",
    "- Multi-model evaluation approach (GPT-4o vs. Claude-3.5)\n",
    "- Structured prompt engineering with explicit evaluation criteria\n",
    "- Temperature settings optimized for consistent assessment\n",
    "- Blind evaluation (models unaware of generation source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "30c4a5a6-78cf-4cfc-85a1-f82d1f1b0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IndividualMetricEvaluator:    \n",
    "    def __init__(self, model: str = \"gpt-4o\", temperature: float = 0, language: str = \"dutch\"):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.language = language.lower()\n",
    "        \n",
    "        if self.language not in [\"dutch\", \"english\"]:\n",
    "            raise ValueError(\"Language must be either 'dutch' or 'english'\")\n",
    "        \n",
    "        if model.startswith(\"claude\"):\n",
    "            self.llm = ChatAnthropic(\n",
    "                model_name=model, \n",
    "                temperature=temperature,\n",
    "            )\n",
    "        else:  \n",
    "            self.llm = ChatOpenAI(model_name=model, temperature=temperature)\n",
    "    \n",
    "    def _evaluate_language_level(self, listing: str, applicant_profile: str = \"\") -> int:\n",
    "        \"\"\"Evaluate if language level is appropriate for potential applicants\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een expert in taalgebruik en recruitmentcommunicatie.\n",
    "\n",
    "Evalueer deze vacaturetekst op taalniveau en geschiktheid voor de doelgroep.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "DOELGROEP PROFIEL: {applicant_profile}\n",
    "\n",
    "EVALUATIECRITERIUM: Het taalgebruik in de vacaturetekst is passend voor de beoogde doelgroep.\n",
    "\n",
    "Beoordeel specifiek:\n",
    "- Is de woordkeuze geschikt voor het opleidingsniveau van kandidaten?\n",
    "- Is de complexiteit van zinnen passend?\n",
    "- Wordt vakjargon op het juiste niveau gebruikt?\n",
    "- Is de tekst toegankelijk voor de beoogde kandidaten?\n",
    "\n",
    "Geef een score volgens de schaal:\n",
    "1 = Helemaal mee oneens (taal helemaal niet passend)\n",
    "2 = Oneens (taal grotendeels ongeschikt)\n",
    "3 = Voldoende (taal redelijk geschikt)\n",
    "4 = Eens (taal goed geschikt)\n",
    "5 = Helemaal mee eens (taal perfect geschikt)\n",
    "\n",
    "Antwoord alleen met het cijfer (1, 2, 3, 4, of 5):\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are an expert in language use and recruitment communication.\n",
    "\n",
    "Evaluate this job listing on language level and suitability for the target audience.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "TARGET AUDIENCE PROFILE: {applicant_profile}\n",
    "\n",
    "EVALUATION CRITERION: The language used in the job listing is appropriate for the potential applicants.\n",
    "\n",
    "Assess specifically:\n",
    "- Is the vocabulary suitable for the education level of candidates?\n",
    "- Is the sentence complexity appropriate?\n",
    "- Is professional jargon used at the right level?\n",
    "- Is the text accessible to the intended candidates?\n",
    "\n",
    "Give a score according to the scale:\n",
    "1 = Strongly disagree (language completely inappropriate)\n",
    "2 = Somewhat disagree (language largely inappropriate)\n",
    "3 = Satisfactory (language reasonably appropriate)\n",
    "4 = Somewhat agree (language well-suited)\n",
    "5 = Strongly agree (language perfectly appropriate)\n",
    "\n",
    "Answer only with the number (1, 2, 3, 4, or 5):\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_syntax_grammar(self, listing: str, syntax_rules: str = \"\") -> int:\n",
    "        \"\"\"Evaluate adherence to grammar and syntax conventions\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een taalexpert gespecialiseerd in zakelijke communicatie en recruitment.\n",
    "\n",
    "Evalueer deze vacaturetekst op grammatica en zinsbouwregels.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "GRAMMATICA- EN ZINSBOUWREGELS VAN HET RECRUITMENTBEDRIJF:\n",
    "{syntax_rules}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst volgt de grammatica- en zinsbouwregels zoals vastgesteld door het recruitmentbedrijf.\n",
    "\n",
    "Beoordeel met focus op:\n",
    "- Correcte grammatica volgens Nederlandse taalregels\n",
    "- Naleving van bedrijfsspecifieke stijlregels\n",
    "- Juiste zinsbouw en interpunctie\n",
    "- Consistentie in taalgebruik\n",
    "\n",
    "Let op: Beoordeel realistisch - kleine onvolkomenheden hoeven niet tot een lage score te leiden als de communicatie helder blijft.\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (veel grammaticafouten)\n",
    "2 = Oneens (enkele fouten maar nog leesbaar)\n",
    "3 = Voldoende (acceptabel niveau, kleine onvolkomenheden)\n",
    "4 = Eens (goed geschreven)\n",
    "5 = Helemaal mee eens (uitstekende grammatica en zinsbouw)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are a language expert specialized in business communication and recruitment.\n",
    "\n",
    "Evaluate this job listing on grammar and syntax conventions.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "GRAMMAR AND SYNTAX RULES FROM THE RECRUITMENT COMPANY:\n",
    "{syntax_rules}\n",
    "\n",
    "EVALUATION CRITERION: The job listing adheres to the grammar and syntax conventions set by the recruitment company.\n",
    "\n",
    "Assess with focus on:\n",
    "- Correct grammar according to English language rules\n",
    "- Compliance with company-specific style rules\n",
    "- Proper sentence structure and punctuation\n",
    "- Consistency in language use\n",
    "\n",
    "Note: Assess realistically - minor imperfections should not lead to a low score if communication remains clear.\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (many grammar errors)\n",
    "2 = Somewhat disagree (some errors but still readable)\n",
    "3 = Satisfactory (acceptable level, minor imperfections)\n",
    "4 = Somewhat agree (well-written)\n",
    "5 = Strongly agree (excellent grammar and syntax)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_motivating_text(self, listing: str) -> int:\n",
    "        \"\"\"Evaluate effectiveness in motivating candidates to apply\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een recruitment specialist met expertise in kandidaatmotivatie.\n",
    "\n",
    "Evalueer deze vacaturetekst op motiverende werking.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst motiveert potentiële kandidaten op effectieve wijze om te solliciteren op deze functie.\n",
    "\n",
    "Beoordeel specifiek:\n",
    "- Wordt de functie aantrekkelijk gepresenteerd?\n",
    "- Creëert de tekst enthousiasme voor de positie?\n",
    "- Worden voordelen en kansen duidelijk gecommuniceerd?\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (niet motiverend)\n",
    "2 = Oneens (beperkt motiverend)\n",
    "3 = Voldoende (redelijk motiverend)\n",
    "4 = Eens (goed motiverend)\n",
    "5 = Helemaal mee eens (zeer effectief motiverend)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are a recruitment specialist with expertise in candidate motivation.\n",
    "\n",
    "Evaluate this job listing on motivational effectiveness.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "EVALUATION CRITERION: The job listing effectively motivates potential candidates to apply for this position.\n",
    "\n",
    "Assess specifically:\n",
    "- Is the position presented attractively?\n",
    "- Does the text create enthusiasm for the position?\n",
    "- Are benefits and opportunities clearly communicated?\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (not motivating)\n",
    "2 = Somewhat disagree (limited motivation)\n",
    "3 = Satisfactory (reasonably motivating)\n",
    "4 = Somewhat agree (well-motivating)\n",
    "5 = Strongly agree (very effectively motivating)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_creativity_level(self, listing: str) -> int:\n",
    "        \"\"\"Evaluate appropriate level of creativity and originality\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een expert in creatieve communicatie binnen de recruitment sector.\n",
    "\n",
    "Evalueer deze vacaturetekst op creativiteitsniveau en originaliteit.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst toont een passend niveau van creativiteit en originaliteit in de presentatie.\n",
    "\n",
    "Beoordeel met recruitment context in gedachten:\n",
    "- Is de presentatie origineel en onderscheidend?\n",
    "- Past het creativiteitsniveau bij het type functie?\n",
    "- Maakt creativiteit de tekst aantrekkelijker zonder onduidelijkheid?\n",
    "\n",
    "Let op: Vacatureteksten hoeven niet extreem creatief te zijn - helderheid en functionaliteit zijn belangrijker dan pure originaliteit.\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (zeer droog, geen creativiteit)\n",
    "2 = Oneens (overwegend standaard)\n",
    "3 = Voldoende (evenwichtige mix van helderheid en aantrekkelijkheid)\n",
    "4 = Eens (goed gebruik van creatieve elementen)\n",
    "5 = Helemaal mee eens (uitstekend creatief binnen professionele context)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are an expert in creative communication within the recruitment sector.\n",
    "\n",
    "Evaluate this job listing on creativity level and originality.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "EVALUATION CRITERION: The job listing demonstrates an appropriate level of creativity and originality in its presentation.\n",
    "\n",
    "Assess with recruitment context in mind:\n",
    "- Is the presentation original and distinctive?\n",
    "- Does the creativity level match the type of position?\n",
    "- Does creativity make the text more attractive without causing confusion?\n",
    "\n",
    "Note: Job listings don't need to be extremely creative - clarity and functionality are more important than pure originality.\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (very dry, no creativity)\n",
    "2 = Somewhat disagree (predominantly standard)\n",
    "3 = Satisfactory (balanced mix of clarity and attractiveness)\n",
    "4 = Somewhat agree (good use of creative elements)\n",
    "5 = Strongly agree (excellently creative within professional context)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_content_rules(self, listing: str, content_rules: str = \"\") -> int:\n",
    "        \"\"\"Evaluate inclusion of all content rules set by recruitment company\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een compliance expert voor recruitment communicatie.\n",
    "\n",
    "Evalueer deze vacaturetekst op naleving van inhoudsregels.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "INHOUDSREGELS VAN HET RECRUITMENTBEDRIJF:\n",
    "{content_rules}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst voldoet aan alle inhoudsregels die zijn opgesteld door het recruitmentbedrijf.\n",
    "\n",
    "Controleer systematisch:\n",
    "- Zijn alle verplichte inhoudselementen aanwezig?\n",
    "- Is alle voorgeschreven informatie correct geintegreerd?\n",
    "\n",
    "\n",
    "Beoordeel strikt op completheid van de regelnaleving.\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (geen van de regels gevolgd)\n",
    "2 = Oneens (enkele regels maar veel ontbreekt)\n",
    "3 = Voldoende (meeste regels gevolgd, kleine tekortkomingen)\n",
    "4 = Eens (bijna alle regels correct gevolgd)\n",
    "5 = Helemaal mee eens (alle inhoudsregels perfect nageleefd)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are a compliance expert for recruitment communication.\n",
    "\n",
    "Evaluate this job listing on adherence to content rules.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "CONTENT RULES FROM THE RECRUITMENT COMPANY:\n",
    "{content_rules}\n",
    "\n",
    "EVALUATION CRITERION: The job listing includes all content rules set by the recruitment company.\n",
    "\n",
    "Check systematically:\n",
    "- Are all mandatory content elements present?\n",
    "- Is all prescribed information correctly incorporated?\n",
    "\n",
    "Assess strictly on completeness of rule compliance.\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (none of the rules followed)\n",
    "2 = Somewhat disagree (some rules but much is missing)\n",
    "3 = Satisfactory (most rules followed, minor shortcomings)\n",
    "4 = Somewhat agree (almost all rules correctly followed)\n",
    "5 = Strongly agree (all content rules perfectly adhered to)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_tone_of_voice(self, listing: str, tone_of_voice: str = \"\") -> int:\n",
    "        \"\"\"Evaluate consistent adherence to specified tone of voice\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een brand communication specialist.\n",
    "\n",
    "Evalueer deze vacaturetekst op tone of voice consistentie.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "VOORGESCHREVEN TONE OF VOICE VAN HET RECRUITMENTBEDRIJF:\n",
    "{tone_of_voice}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst houdt consequent de voorgeschreven tone of voice aan, passend bij de beoogde communicatiestijl.\n",
    "\n",
    "Beoordeel specifiek:\n",
    "- Is de tone of voice consistent door de hele tekst?\n",
    "- Zijn er afwijkingen van de voorgeschreven stijl?\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (houdt zich niet aan tone of voice)\n",
    "2 = Oneens (beperkte aansluiting)\n",
    "3 = Voldoende (redelijke aansluiting)\n",
    "4 = Eens (goede aansluiting)\n",
    "5 = Helemaal mee eens (perfecte aansluiting bij tone of voice)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are a brand communication specialist.\n",
    "\n",
    "Evaluate this job listing on tone of voice consistency.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "PRESCRIBED TONE OF VOICE FROM THE RECRUITMENT COMPANY:\n",
    "{tone_of_voice}\n",
    "\n",
    "EVALUATION CRITERION: The job listing consistently adheres to the specified tone of voice, aligning with the intended communicative style.\n",
    "\n",
    "Assess specifically:\n",
    "- Is the tone of voice consistent throughout the text?\n",
    "- Are there deviations from the prescribed style?\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (does not adhere to tone of voice)\n",
    "2 = Somewhat disagree (limited alignment)\n",
    "3 = Satisfactory (reasonable alignment)\n",
    "4 = Somewhat agree (good alignment)\n",
    "5 = Strongly agree (perfect alignment with tone of voice)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_applicant_profile(self, listing: str, applicant_profile: str = \"\") -> int:\n",
    "        \"\"\"Evaluate reflection of competencies for target candidate\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een recruitment consultant gespecialiseerd in candidate profiling.\n",
    "\n",
    "Evalueer deze vacaturetekst op reflectie van competenties voor de doelkandidaat.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "PROFIEL VAN DE BEOOGDE KANDIDAAT:\n",
    "{applicant_profile}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst weerspiegelt de competenties voor de beoogde kandidaat.\n",
    "\n",
    "Beoordeel specifiek:\n",
    "- Worden de vereiste competenties duidelijk weergegeven?\n",
    "- Worden relevante verantwoordelijkheden genoemd?\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (competenties helemaal niet weerspiegeld)\n",
    "2 = Oneens (beperkte weerspiegeling)\n",
    "3 = Voldoende (redelijke weerspiegeling)\n",
    "4 = Eens (goede weerspiegeling)\n",
    "5 = Helemaal mee eens (perfecte weerspiegeling van competenties)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are a recruitment consultant specialized in candidate profiling.\n",
    "\n",
    "Evaluate this job listing on reflection of competencies for the target candidate.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "PROFILE OF THE TARGET CANDIDATE:\n",
    "{applicant_profile}\n",
    "\n",
    "EVALUATION CRITERION: The job listing reflects the competencies for the target candidate.\n",
    "\n",
    "Assess specifically:\n",
    "- Are the required competencies clearly represented?\n",
    "- Are relevant responsibilities mentioned?\n",
    "\n",
    "Score according scale:\n",
    "1 = Strongly disagree (competencies not reflected at all)\n",
    "2 = Somewhat disagree (limited reflection)\n",
    "3 = Satisfactory (reasonable reflection)\n",
    "4 = Somewhat agree (good reflection)\n",
    "5 = Strongly agree (perfect reflection of competencies)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return self._extract_score(response.content)\n",
    "    \n",
    "    def _evaluate_company_information(self, listing: str) -> int:\n",
    "        \"\"\"Evaluate provision of contextual, relevant company information\"\"\"\n",
    "        if self.language == \"dutch\":\n",
    "            prompt = f\"\"\"\n",
    "Je bent een employer branding specialist.\n",
    "\n",
    "Evalueer deze vacaturetekst op bedrijfsinformatie.\n",
    "\n",
    "VACATURETEKST:\n",
    "{listing}\n",
    "\n",
    "EVALUATIECRITERIUM: De vacaturetekst bevat contextuele en relevante informatie over het bedrijf.\n",
    "\n",
    "Beoordeel specifiek:\n",
    "- Is er voldoende informatie over het bedrijf aanwezig?\n",
    "- Is de bedrijfsinformatie relevant?\n",
    "- Geeft de tekst een goed beeld van de organisatie?\n",
    "\n",
    "Score volgens schaal:\n",
    "1 = Helemaal mee oneens (geen relevante bedrijfsinformatie)\n",
    "2 = Oneens (zeer beperkte informatie)\n",
    "3 = Voldoende (basis bedrijfsinformatie aanwezig)\n",
    "4 = Eens (goede contextuele informatie)\n",
    "5 = Helemaal mee eens (uitstekende, relevante bedrijfsinformatie)\n",
    "\n",
    "Score: \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "You are an employer branding specialist.\n",
    "\n",
    "Evaluate this job listing on company information.\n",
    "\n",
    "JOB LISTING:\n",
    "{listing}\n",
    "\n",
    "EVALUATION CRITERION: The job listing provides contextual, relevant information about the company.\n",
    "\n",
    "Assess specifically:\n",
    "- Is there sufficient information about the company present?\n",
    "- Is the company information relevant?\n",
    "- Does the text provide a good picture of the organization?\n",
    "\n",
    "Score according to scale:\n",
    "1 = Strongly disagree (no relevant company information)\n",
    "2 = Somewhat disagree (very limited information)\n",
    "3 = Satisfactory (basic company information present)\n",
    "4 = Somewhat agree (good contextual information)\n",
    "5 = Strongly agree (excellent, relevant company information)\n",
    "\n",
    "Score: \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(response.content)\n",
    "        return self._extract_score(response.content)\n",
    "\n",
    "    def _extract_score(self, response_text: str) -> int:\n",
    "        \"\"\"Extract numerical score from response\"\"\"\n",
    "        patterns = [\n",
    "            r\"score:\\s*(\\d+)\",\n",
    "            r\"(\\d+)\\s*/\\s*5\",\n",
    "            r\"^(\\d+)$\",\n",
    "            r\"(\\d+)\\s*$\",\n",
    "            r\"(\\d+)\\s*=\",\n",
    "            r\"=\\s*(\\d+)\",\n",
    "        ]\n",
    "        \n",
    "        response_lower = response_text.lower().strip()\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response_lower, re.MULTILINE)\n",
    "            if match:\n",
    "                try:\n",
    "                    score = int(match.group(1))\n",
    "                    if 1 <= score <= 5:\n",
    "                        return score\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "        \n",
    "        all_numbers = re.findall(r'\\b([1-5])\\b', response_text)\n",
    "        if all_numbers:\n",
    "            return int(all_numbers[-1])\n",
    "        print(\"Gaat iets fout\")\n",
    "        return 3\n",
    "    \n",
    "    def evaluate_all_metrics(self, listing: str, syntax_rules: str = \"\", \n",
    "                           content_rules: str = \"\", tone_of_voice: str = \"\", \n",
    "                           applicant_profile: str = \"\") -> dict:        \n",
    "        results = {}\n",
    "        \n",
    "\n",
    "        # CLARITTY\n",
    "        results['language_level_score'] = self._evaluate_language_level(\n",
    "            listing, applicant_profile)\n",
    "        \n",
    "        results['syntax_grammar_score'] = self._evaluate_syntax_grammar(\n",
    "            listing, syntax_rules)\n",
    "        \n",
    "        # RELEVANCE\n",
    "        results['motivating_text_score'] = self._evaluate_motivating_text(listing)\n",
    "        \n",
    "        results['creativity_level_score'] = self._evaluate_creativity_level(listing)\n",
    "        \n",
    "        # CORRECTNESS\n",
    "        results['content_rules_score'] = self._evaluate_content_rules(\n",
    "            listing, content_rules)\n",
    "        \n",
    "        results['tone_of_voice_score'] = self._evaluate_tone_of_voice(\n",
    "            listing, tone_of_voice)\n",
    "        \n",
    "        # COMPLETENESS\n",
    "        results['applicant_profile_score'] = self._evaluate_applicant_profile(\n",
    "            listing, applicant_profile)\n",
    "        \n",
    "        results['company_information_score'] = self._evaluate_company_information(listing)\n",
    "        \n",
    "        # Add model_info\n",
    "        results['evaluation_model'] = self.model\n",
    "            \n",
    "        return results\n",
    "\n",
    "def process_dataset_research_framework(df, model: str = \"gpt-4o\", num_rows=None, batch_size=10, language=\"dutch\"):    \n",
    "    evaluator = IndividualMetricEvaluator(model=model, language=language)\n",
    "    results = []\n",
    "    \n",
    "    test_df = df.head(num_rows) if num_rows else df\n",
    "    print(f\"Processing {len(test_df)} job listings with research framework metrics using {model}...\")\n",
    "    \n",
    "    for i in range(0, len(test_df), batch_size):\n",
    "        batch = test_df.iloc[i:i+batch_size]\n",
    "        print(f\"Batch {i//batch_size + 1}/{(len(test_df)-1)//batch_size + 1}\")\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            unique_id_str = str(row['unique_id'])\n",
    "            third_char = unique_id_str[2] if len(unique_id_str) > 2 else '0'\n",
    "            \n",
    "            if third_char == '0':\n",
    "                listing_text = row['job_listing']\n",
    "                listing_type = \"human-written\"\n",
    "            else:\n",
    "                listing_text = row['generated_listing']\n",
    "                listing_type = \"AI-generated\"\n",
    "            \n",
    "            if pd.isna(listing_text) or not str(listing_text).strip():\n",
    "                print(f\"Empty listing for {row['unique_id']}\")\n",
    "                continue\n",
    "            \n",
    "            scores = evaluator.evaluate_all_metrics(\n",
    "                listing=str(listing_text),\n",
    "                syntax_rules=str(row['additional_syntax_rules']) if pd.notna(row['additional_syntax_rules']) else \"\",\n",
    "                content_rules=str(row['additional_content_rules']) if pd.notna(row['additional_content_rules']) else \"\",\n",
    "                tone_of_voice=str(row['tone_of_voice']) if pd.notna(row['tone_of_voice']) else \"\",\n",
    "                applicant_profile=str(row['ideal_candidate_traits']) if pd.notna(row['ideal_candidate_traits']) else \"\"\n",
    "            )\n",
    "            \n",
    "            scores['unique_id'] = row['unique_id']\n",
    "            scores['listing_type'] = listing_type\n",
    "            results.append(scores)\n",
    "            \n",
    "            print(f\"Processed {listing_type} - ID: {row['unique_id']} with {model}\")\n",
    "                \n",
    "\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    final_df = df.merge(results_df, on='unique_id', how='left')\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = model.replace(\":\", \"_\").replace(\"/\", \"_\")\n",
    "    filename = f'job_listings_research_framework_{model_name}_{timestamp}.csv'\n",
    "    final_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Complete! Saved to: {filename}\")\n",
    "    print(f\"Total evaluations: {len(results_df)}\")\n",
    "    \n",
    "\n",
    "    for metric in ['language_level_score', 'syntax_grammar_score', 'motivating_text_score', \n",
    "                   'creativity_level_score', 'content_rules_score', 'tone_of_voice_score',\n",
    "                   'applicant_profile_score', 'company_information_score']:\n",
    "        if metric in results_df.columns:\n",
    "            mean_score = results_df[metric].mean()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dutch evaluations\n",
    "# results_claude = process_dataset_research_framework(nl_check, model=\"claude-3-5-sonnet-20240620\", num_rows=50)\n",
    "# results_gpt = process_dataset_research_framework(nl_check, model=\"gpt-4o-20240513\", num_rows=50)\n",
    "\n",
    "\n",
    "# English evaluations\n",
    "# results_claude_en = process_dataset_research_framework(en_check, model=\"claude-3-5-sonnet-20240620\", num_rows=20, language=\"english\")\n",
    "# results_gpt_en = process_dataset_research_framework(en_check, model=\"gpt-4o-20240513\", num_rows=20, language=\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9610fb-56d4-4b2c-9383-df749ddc365a",
   "metadata": {},
   "source": [
    "## 3.Formatting adjustment for comparison to human raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4df0309a-6def-4472-9390-ee3aa04c4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch = pd.read_excel('/Users/Mick/Master_Thesis/3. evaluation_pipeline/Dutch_surveys_ICR_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e01088f-cb25-4ea6-9057-33491f3eb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_columns = ['language_level_score', 'syntax_grammar_score', 'motivating_text_score',\n",
    "                'creativity_level_score', 'content_rules_score', 'tone_of_voice_score',\n",
    "                'applicant_profile_score', 'company_information_score']\n",
    "\n",
    "\n",
    "new_df = []\n",
    "for idx, row in results_gpt.iterrows():\n",
    "    for response_id, score_col in enumerate(score_columns, 1):\n",
    "        new_df.append({\n",
    "            'ResponseId': response_id,\n",
    "            'unique_id': row['unique_id'],\n",
    "            'LLM_judge_openai': row[score_col]\n",
    "        })\n",
    "\n",
    "df_results_long = pd.DataFrame(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b8691be2-e7f9-4a1a-b108-8653cd48ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch['LLM_judge_openai'] = df_results_long.LLM_judge_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3832b13-1569-43cd-ad7a-43fc3236536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for idx, row in results_claude.iterrows():\n",
    "    for response_id, score_col in enumerate(score_columns, 1):\n",
    "        new_df.append({\n",
    "            'ResponseId': response_id,\n",
    "            'unique_id': row['unique_id'],\n",
    "            'LLM_judge_claude': row[score_col]\n",
    "        })\n",
    "\n",
    "df_results_long = pd.DataFrame(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "721a2307-363c-424a-ab3f-9ddb275450d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch['LLM_judge_claude'] = df_results_long.LLM_judge_claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "789b719c-3759-49d7-ae77-440ff9670f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM_judge_openai</th>\n",
       "      <th>LLM_judge_claude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.7625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LLM_judge_openai  LLM_judge_claude\n",
       "Generated                                    \n",
       "0                       3.3            3.3875\n",
       "1                       3.6            3.7625"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dutch.groupby(\"Generated\")[['LLM_judge_openai',\"LLM_judge_claude\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da32397c-fc2d-4fc4-8539-2c9e8fe56e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch.to_excel('IRR_testing_6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254317a0-fdb1-4999-82ec-fabac43a773b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9839793b-ea5c-4ea5-8d68-516327f93e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = pd.read_excel('/Users/Mick/Master_Thesis/3. evaluation_pipeline/English_surveys_ICR_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1934bc11-1992-4b9d-8d15-db0647de6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for idx, row in results_gpt_en.iterrows():\n",
    "    for response_id, score_col in enumerate(score_columns, 1):\n",
    "        new_df.append({\n",
    "            'ResponseId': response_id,\n",
    "            'unique_id': row['unique_id'],\n",
    "            'LLM_judge_openai': row[score_col]\n",
    "        })\n",
    "\n",
    "df_results_long = pd.DataFrame(new_df)\n",
    "\n",
    "df_english['LLM_judge_openai'] = df_results_long.LLM_judge_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e136d3a-c082-4a2e-a9b3-d61d4f9c91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "for idx, row in results_claude_en.iterrows():\n",
    "    for response_id, score_col in enumerate(score_columns, 1):\n",
    "        new_df.append({\n",
    "            'ResponseId': response_id,\n",
    "            'unique_id': row['unique_id'],\n",
    "            'LLM_judge_claude': row[score_col]\n",
    "        })\n",
    "\n",
    "df_results_long = pd.DataFrame(new_df)\n",
    "df_english['LLM_judge_claude'] = df_results_long.LLM_judge_claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "920e75d6-8170-40e8-a916-fb7c364a575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM_judge_openai</th>\n",
       "      <th>LLM_judge_claude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.125</td>\n",
       "      <td>3.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.675</td>\n",
       "      <td>3.7000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LLM_judge_openai  LLM_judge_claude\n",
       "Generated                                    \n",
       "0                     3.125            3.1125\n",
       "1                     3.675            3.7000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english.groupby(\"Generated\")[['LLM_judge_openai',\"LLM_judge_claude\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "89a5b180-83e6-4123-9195-b7651f27f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.to_excel('IRR_testing_en_2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de5fe1-4467-4851-a17d-6384f2aefb1f",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Implications\n",
    "\n",
    "### Framework Validation\n",
    "This testing validates whether **LLM-based evaluation can replicate expert judgment patterns** while providing:\n",
    "- **Scalable quality assessment** for recruitment applications\n",
    "- **Consistent evaluation standards** across different contexts\n",
    "- **Reduced human evaluation burden** while maintaining quality standards\n",
    "- **Empirical foundation** for automated recruitment content evaluation\n",
    "\n",
    "### Future Research Directions\n",
    "Results inform:\n",
    "- **Metric Prompt refinement** based on agreement patterns\n",
    "- **Sample size planning** for full-scale validation studies  \n",
    "- **Language-specific adaptations** for international recruitment contexts\n",
    "- **Integration strategies** for real-world recruitment workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168af85c-2a78-4dd4-8b6d-8e40407f7ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405ea9-33fd-4967-b7af-34f88b9dda74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0837a-dea4-4dfe-9d9a-02578c9a0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98f490-489e-41bf-855e-4cea8489325b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c72100-744b-496f-b943-9cbe4f4c6cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
