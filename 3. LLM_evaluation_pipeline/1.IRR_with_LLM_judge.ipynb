{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c310c4-3b59-4408-b66d-e0a1ead04279",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Inter-rater Reliability Analysis\n",
    "\n",
    "## Framework Validation: Human-LLM Agreement Study\n",
    "\n",
    "This notebook presents the second component of the framework validation: assessing the reliability of LLM-based evaluation compared to human expert judgments. Following the human-to-human reliability analysis, this experiment evaluates whether LLMs can consistently assess job listing quality according to expert-defined standards.\n",
    "\n",
    "### Research Context\n",
    "- **Objective**: Validate LLM-as-a-judge reliability against human expert evaluations\n",
    "- **Models**: GPT-4o and Claude-3.5-Sonnet as independent evaluators\n",
    "- **Methodology**: Gwet's $AC_1$ coefficient and raw agreement analysis\n",
    "- **Significance**: Establishes automated evaluation capabilities for the framework\n",
    "\n",
    "### Dual-Model Approach\n",
    "The experiment employs two leading LLMs to mitigate potential self-enhancement bias and provide robust comparative analysis of automated evaluation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f6a1b088-4ee2-4610-a5e2-77907401c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pwr)\n",
    "library(irr)\n",
    "library(readxl)\n",
    "library(dplyr)\n",
    "library(irrCAC)\n",
    "library(knitr)\n",
    "library(kableExtra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18acfd-63bc-49e1-b3bc-6831ccff26a4",
   "metadata": {},
   "source": [
    "## 1. Dutch Sample Analysis\n",
    "\n",
    "### Data Loading and Preprocessing\n",
    "Loading expert evaluation data alongside LLM judgments for Dutch job listings. The dataset contains human expert ratings and corresponding LLM evaluations using identical prompts and scoring criteria.\n",
    "\n",
    "**Data Structure:**\n",
    "- Human expert ratings: Two recruitment professionals\n",
    "- LLM evaluations: GPT-4o and Claude-3.5 assessments\n",
    "- Evaluation criteria: Same 8 quality metrics used in human-only analysis\n",
    "- Binary classification: ≥3 threshold for acceptable quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "da0d5341-3fc3-4d54-a156-c21936d88961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m•\u001b[39m `` -> `...1`\n"
     ]
    }
   ],
   "source": [
    "df_dutch <- read_excel(\"IRR_testing_5.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9c14707-93a0-4833-b8f6-5f312a99dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dutch <- df_dutch %>%\n",
    "  rename(\n",
    "    rater1 = R_8q7mN6gv7GC7sxd,\n",
    "    rater2 = R_2wT7oFicvVDdz9L\n",
    "  )\n",
    "\n",
    "df_dutch <- df_dutch %>%\n",
    "  mutate(\n",
    "    rater1 = as.numeric(rater1),\n",
    "    rater2 = as.numeric(rater2)\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "binarize <- function(x) ifelse(x >= 3, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07128bf3-059e-4112-ae6b-d43f4fc5c1d3",
   "metadata": {},
   "source": [
    "### Comprehensive Reliability Analysis Framework\n",
    "\n",
    "The analysis framework computes multiple reliability metrics to assess LLM-as-a-judge performance:\n",
    "\n",
    "**Core Metrics:**\n",
    "- **AC1 Coefficients**: Primary reliability measure between raters\n",
    "- **Raw Agreement**: Proportion of identical classifications\n",
    "- **Bias Index**: Systematic rating differences between evaluators\n",
    "- **Sample Size Requirements**: Statistical power calculations for validation\n",
    "\n",
    "**Comparison Strategy:**\n",
    "- Human-to-human baseline reliability\n",
    "- Each LLM compared individually against human experts\n",
    "- Direct LLM-to-LLM agreement assessment\n",
    "- Performance-based model selection criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "20a47b31-4487-4a51-8b95-a530acfdc50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUTCH RESULTS ===\n",
      "\n",
      "\u001b[90m# A tibble: 8 × 5\u001b[39m\n",
      "  ResponseId prevalence pabak cohens_kappa gwet_ac1\n",
      "       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \n",
      "\u001b[90m1\u001b[39m          1 0.93       0.70  -0.07        0.83    \n",
      "\u001b[90m2\u001b[39m          2 0.90       0.60  -0.11        0.76    \n",
      "\u001b[90m3\u001b[39m          3 0.80       0.20  0.00         0.41    \n",
      "\u001b[90m4\u001b[39m          4 0.70       0.60  0.55         0.66    \n",
      "\u001b[90m5\u001b[39m          5 0.70       0.60  0.53         0.66    \n",
      "\u001b[90m6\u001b[39m          6 0.80       0.40  0.12         0.56    \n",
      "\u001b[90m7\u001b[39m          7 0.88       0.50  0.00         0.68    \n",
      "\u001b[90m8\u001b[39m          8 0.78       0.70  0.57         0.77    \n"
     ]
    }
   ],
   "source": [
    "calculate_all_metrics <- function(df, group_var) {\n",
    " df %>%\n",
    "   group_by(!!sym(group_var)) %>%\n",
    "   summarise(\n",
    "     prevalence = {\n",
    "       r1 <- binarize(rater1)\n",
    "       r2 <- binarize(rater2)\n",
    "       (mean(r1) + mean(r2)) / 2\n",
    "     },\n",
    "     pabak = {\n",
    "       r1 <- binarize(rater1)\n",
    "       r2 <- binarize(rater2)\n",
    "       po <- mean(r1 == r2)\n",
    "       2 * po - 1\n",
    "     },\n",
    "     cohens_kappa = {\n",
    "       r1 <- binarize(rater1)\n",
    "       r2 <- binarize(rater2)\n",
    "       ratings <- data.frame(r1, r2)\n",
    "       k <- kappa2(ratings, \"unweighted\")\n",
    "       k$value\n",
    "     },\n",
    "     gwet_ac1 = {\n",
    "       r1 <- binarize(rater1)\n",
    "       r2 <- binarize(rater2)\n",
    "       ratings <- data.frame(r1, r2)\n",
    "       ac1 <- gwet.ac1.raw(ratings)\n",
    "       ac1$est$coeff.val\n",
    "     },\n",
    "     .groups = 'drop'\n",
    "   )\n",
    "}\n",
    "\n",
    "results_dutch <- calculate_all_metrics(df_dutch, \"ResponseId\")\n",
    "results_dutch_display <- results_dutch %>%\n",
    " mutate(\n",
    "   prevalence = sprintf(\"%.2f\", prevalence),\n",
    "   pabak = sprintf(\"%.2f\", pabak),\n",
    "   cohens_kappa = sprintf(\"%.2f\", cohens_kappa),\n",
    "   gwet_ac1 = sprintf(\"%.2f\", gwet_ac1)\n",
    " )\n",
    "\n",
    "cat(\"=== DUTCH RESULTS ===\\n\\n\")\n",
    "print(results_dutch_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88c966-ff57-4f08-8c86-f6192474d7e6",
   "metadata": {},
   "source": [
    "### Dutch Sample Results\n",
    "\n",
    "**Performance Summary:**\n",
    "The tables below present comprehensive reliability metrics for Dutch job listing evaluations, comparing human expert agreement with LLM-based assessments across all quality dimensions.\n",
    "\n",
    "**Interpretation Guidelines:**\n",
    "- AC1 ≥ 0.61: Substantial agreement (framework deployment ready)\n",
    "- Low Bias Index: Minimal systematic rating differences\n",
    "- Best LLM determination: Based on agreement and bias performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4fe2968d-0f1d-43d8-9cd0-a7798e0800a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_agreement <- function(rater1, rater2) {\n",
    " if (length(rater1) != length(rater2)) return(NA_real_)\n",
    " mean(rater1 == rater2, na.rm = TRUE)\n",
    "}\n",
    "\n",
    "calculate_ac1_metrics <- function(df, group_var) {\n",
    " df %>%\n",
    "   group_by(!!sym(group_var)) %>%\n",
    "   summarise(\n",
    "     ac1_humans = {\n",
    "       r1 <- binarize(rater1)\n",
    "       r2 <- binarize(rater2)\n",
    "       gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "     },\n",
    "     \n",
    "     raw_agreement_humans = raw_agreement(binarize(rater1), binarize(rater2)),\n",
    "     \n",
    "     ac1_openai_vs_average_humans = mean(c(\n",
    "       {\n",
    "         r1 <- binarize(rater1)\n",
    "         r2 <- binarize(LLM_judge_openai)\n",
    "         gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "       },\n",
    "       {\n",
    "         r1 <- binarize(rater2)\n",
    "         r2 <- binarize(LLM_judge_openai)\n",
    "         gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "       }\n",
    "     ), na.rm = TRUE),\n",
    "     \n",
    "     raw_agreement_openai_vs_average_humans = mean(c(\n",
    "       raw_agreement(binarize(rater1), binarize(LLM_judge_openai)),\n",
    "       raw_agreement(binarize(rater2), binarize(LLM_judge_openai))\n",
    "     ), na.rm = TRUE),\n",
    "     \n",
    "     sample_size_openai_vs_humans = kappaSize.ac1(k0 = 0, k1 = ac1_openai_vs_average_humans, alpha = 0.05, power = 0.8)$N,\n",
    "     \n",
    "     bias_openai_vs_average_humans = (mean(binarize(rater1)) + mean(binarize(rater2))) / 2 - mean(binarize(LLM_judge_openai)),\n",
    "     \n",
    "     ac1_claude_vs_average_humans = mean(c(\n",
    "       {\n",
    "         r1 <- binarize(rater1)\n",
    "         r2 <- binarize(LLM_judge_claude)\n",
    "         gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "       },\n",
    "       {\n",
    "         r1 <- binarize(rater2)\n",
    "         r2 <- binarize(LLM_judge_claude)\n",
    "         gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "       }\n",
    "     ), na.rm = TRUE),\n",
    "     \n",
    "     raw_agreement_claude_vs_average_humans = mean(c(\n",
    "       raw_agreement(binarize(rater1), binarize(LLM_judge_claude)),\n",
    "       raw_agreement(binarize(rater2), binarize(LLM_judge_claude))\n",
    "     ), na.rm = TRUE),\n",
    "     \n",
    "     bias_claude_vs_average_humans = (mean(binarize(rater1)) + mean(binarize(rater2))) / 2 - mean(binarize(LLM_judge_claude)),\n",
    "     \n",
    "     ac1_openai_vs_claude = {\n",
    "       r1 <- binarize(LLM_judge_openai)\n",
    "       r2 <- binarize(LLM_judge_claude)\n",
    "       gwet.ac1.raw(data.frame(r1, r2))$est$coeff.val\n",
    "     },\n",
    "     \n",
    "     raw_agreement_openai_vs_claude = raw_agreement(binarize(LLM_judge_openai), binarize(LLM_judge_claude)),\n",
    "     \n",
    "     bias_openai_vs_claude = mean(binarize(LLM_judge_openai)) - mean(binarize(LLM_judge_claude)),\n",
    "     \n",
    "     best_llm_by_agreement = case_when(\n",
    "       is.na(ac1_openai_vs_average_humans) & is.na(ac1_claude_vs_average_humans) ~ \"Tie (both NA)\",\n",
    "       is.na(ac1_openai_vs_average_humans) ~ \"Claude\",\n",
    "       is.na(ac1_claude_vs_average_humans) ~ \"OpenAI\",\n",
    "       ac1_openai_vs_average_humans > ac1_claude_vs_average_humans ~ \"OpenAI\",\n",
    "       ac1_claude_vs_average_humans > ac1_openai_vs_average_humans ~ \"Claude\",\n",
    "       TRUE ~ \"Tie\"\n",
    "     ),\n",
    "     \n",
    "     best_llm_by_bias = case_when(\n",
    "       is.na(bias_openai_vs_average_humans) & is.na(bias_claude_vs_average_humans) ~ \"Tie (both NA)\",\n",
    "       is.na(bias_openai_vs_average_humans) ~ \"Claude\",\n",
    "       is.na(bias_claude_vs_average_humans) ~ \"OpenAI\",\n",
    "       abs(bias_openai_vs_average_humans) < abs(bias_claude_vs_average_humans) ~ \"OpenAI\",\n",
    "       abs(bias_claude_vs_average_humans) < abs(bias_openai_vs_average_humans) ~ \"Claude\",\n",
    "       TRUE ~ \"Tie\"\n",
    "     ),\n",
    "     \n",
    "     best_llm_overall = case_when(\n",
    "       best_llm_by_agreement == best_llm_by_bias & !grepl(\"Tie\", best_llm_by_agreement) ~ best_llm_by_agreement,\n",
    "       grepl(\"Tie\", best_llm_by_agreement) & grepl(\"Tie\", best_llm_by_bias) ~ \"Tie\",\n",
    "       grepl(\"Tie\", best_llm_by_agreement) & !grepl(\"Tie\", best_llm_by_bias) ~ best_llm_by_bias,\n",
    "       !grepl(\"Tie\", best_llm_by_agreement) & grepl(\"Tie\", best_llm_by_bias) ~ best_llm_by_agreement,\n",
    "       TRUE ~ \"Tie\"\n",
    "     ),\n",
    "     \n",
    "     .groups = \"drop\"\n",
    "   )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06e925ea-11d3-4684-bf5a-27df48133ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dutch <- calculate_ac1_metrics(df_dutch, \"ResponseId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "032bf4de-f8a5-4a97-a983-6ccd86335667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: AC1 Agreement Coefficients\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |Best LLM |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|:--------|\n",
       "|      1      |    0.83     |       0.92       |       0.92       |       1.00       |Tie      |\n",
       "|      2      |    0.76     |       0.89       |       0.89       |       1.00       |Tie      |\n",
       "|      3      |    0.41     |       0.71       |       0.71       |       1.00       |Tie      |\n",
       "|      4      |    0.66     |       0.58       |       0.58       |       1.00       |Tie      |\n",
       "|      5      |    0.66     |       0.59       |       0.59       |       1.00       |Tie      |\n",
       "|      6      |    0.56     |       0.79       |       0.71       |       0.87       |Tie      |\n",
       "|      7      |    0.68     |       0.84       |       0.84       |       1.00       |Tie      |\n",
       "|      8      |    0.77     |       0.80       |       0.79       |       0.91       |OpenAI   |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: Raw Agreement Proportions\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|\n",
       "|      1      |    0.85     |       0.92       |       0.92       |       1.00       |\n",
       "|      2      |    0.80     |       0.90       |       0.90       |       1.00       |\n",
       "|      3      |    0.60     |       0.80       |       0.80       |       1.00       |\n",
       "|      4      |    0.80     |       0.70       |       0.70       |       1.00       |\n",
       "|      5      |    0.80     |       0.70       |       0.70       |       1.00       |\n",
       "|      6      |    0.70     |       0.85       |       0.80       |       0.90       |\n",
       "|      7      |    0.75     |       0.88       |       0.88       |       1.00       |\n",
       "|      8      |    0.85     |       0.88       |       0.88       |       0.95       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: LLM Performance Comparison and Decision Rationale (Raw Agreement)\n",
       "\n",
       "| Response ID | OpenAI Raw Agreement | Claude Raw Agreement | OpenAI &#124;Bias&#124; | Claude &#124;Bias&#124; |Agreement Winner |Bias Winner |Overall Winner |\n",
       "|:-----------:|:--------------------:|:--------------------:|:-----------------------:|:-----------------------:|:----------------|:-----------|:--------------|\n",
       "|      1      |         0.92         |         0.92         |          0.07           |          0.07           |Tie              |Tie         |Tie            |\n",
       "|      2      |         0.90         |         0.90         |          0.10           |          0.10           |Tie              |Tie         |Tie            |\n",
       "|      3      |         0.80         |         0.80         |          0.20           |          0.20           |Tie              |Tie         |Tie            |\n",
       "|      4      |         0.70         |         0.70         |          0.30           |          0.30           |Tie              |Tie         |Tie            |\n",
       "|      5      |         0.70         |         0.70         |          0.30           |          0.30           |Tie              |Tie         |Tie            |\n",
       "|      6      |         0.85         |         0.80         |          0.10           |          0.00           |OpenAI           |Claude      |Tie            |\n",
       "|      7      |         0.88         |         0.88         |          0.03           |          0.03           |Tie              |Tie         |Tie            |\n",
       "|      8      |         0.88         |         0.88         |          0.08           |          0.12           |OpenAI           |OpenAI      |OpenAI         |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dutch %>%\n",
    "  select(ResponseId, ac1_humans, ac1_openai_vs_average_humans, \n",
    "         ac1_claude_vs_average_humans, ac1_openai_vs_claude, best_llm_overall) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\", \"Best LLM\"),\n",
    "    caption = \"AC1 Agreement Coefficients\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"l\"),\n",
    "    digits = 2\n",
    "  )\n",
    "\n",
    "\n",
    "results_dutch %>%\n",
    "  select(ResponseId, raw_agreement_humans, raw_agreement_openai_vs_average_humans,\n",
    "         raw_agreement_claude_vs_average_humans, raw_agreement_openai_vs_claude) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\"),\n",
    "    caption = \"Raw Agreement Proportions\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\"),\n",
    "    digits = 2\n",
    "  )\n",
    "\n",
    "\n",
    "results_dutch %>%\n",
    "  mutate(\n",
    "    abs_bias_openai = abs(bias_openai_vs_average_humans),\n",
    "    abs_bias_claude = abs(bias_claude_vs_average_humans)\n",
    "  ) %>%\n",
    "  select(ResponseId, \n",
    "         raw_agreement_openai_vs_average_humans, raw_agreement_claude_vs_average_humans,\n",
    "         abs_bias_openai, abs_bias_claude,\n",
    "         best_llm_by_agreement, best_llm_by_bias, best_llm_overall) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \n",
    "                  \"OpenAI Raw Agreement\", \"Claude Raw Agreement\",\n",
    "                  \"OpenAI |Bias|\", \"Claude |Bias|\",\n",
    "                  \"Agreement Winner\", \"Bias Winner\", \"Overall Winner\"),\n",
    "    caption = \"LLM Performance Comparison and Decision Rationale (Raw Agreement)\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"l\", \"l\", \"l\"),\n",
    "    digits = 2\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac14915-6f5f-4ef7-9246-e01cb7b2bffe",
   "metadata": {},
   "source": [
    "## 2. English Sample Analysis\n",
    "\n",
    "### Cross-Language Validation\n",
    "Extending the LLM-as-a-judge analysis to English job listings to assess framework generalizability across languages and cultural contexts.\n",
    "\n",
    "**Methodological Consistency:**\n",
    "- Identical evaluation procedures as Dutch analysis\n",
    "- Same LLM models and prompt structures\n",
    "- Equivalent human expert selection criteria\n",
    "- Consistent binary classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aaa3bb9f-61e9-470e-9929-7c2b5229c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m•\u001b[39m `` -> `...1`\n"
     ]
    }
   ],
   "source": [
    "df_english <- read_excel(\"IRR_testing_en_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ed06f386-687c-4bb2-9cf6-b53e8b3191db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english <- df_english %>%\n",
    "  rename(\n",
    "    rater1 = R_2F4IWrRogs2MEvL,\n",
    "    rater2 = R_2rkb0tBGzURC0Ia\n",
    "  )\n",
    "\n",
    "df_english <- df_english %>%\n",
    "  mutate(\n",
    "    rater1 = as.numeric(rater1),\n",
    "    rater2 = as.numeric(rater2)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b60836f2-17e5-4c40-90f9-de234263aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_english <- calculate_ac1_metrics(df_english, \"ResponseId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660d28b-45dd-4454-b7ee-c3fb293bbb2c",
   "metadata": {},
   "source": [
    "### English Sample Results\n",
    "\n",
    "**Language-Specific Performance:**\n",
    "The following analysis examines whether LLM-based evaluation maintains reliability across different languages, addressing framework scalability for international recruitment contexts.\n",
    "\n",
    "**Comparative Focus:**\n",
    "- English vs. Dutch reliability patterns\n",
    "- Model performance consistency across languages\n",
    "- Cultural/linguistic bias detection in automated evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "73ea9fd9-79fd-42ec-adb3-73420b3f4415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: AC1 Agreement Coefficients\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |Best LLM |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|:--------|\n",
       "|      1      |    0.60     |       0.81       |       0.81       |       1.00       |Tie      |\n",
       "|      2      |    0.41     |       0.74       |       0.74       |       1.00       |Tie      |\n",
       "|      3      |    0.36     |       0.66       |       0.68       |       0.95       |Tie      |\n",
       "|      4      |    0.04     |       0.53       |       0.45       |       0.92       |OpenAI   |\n",
       "|      5      |    0.65     |       0.75       |       0.73       |       0.92       |OpenAI   |\n",
       "|      6      |    0.51     |       0.79       |       0.69       |       0.83       |Tie      |\n",
       "|      7      |    -0.10    |       0.48       |       0.48       |       1.00       |Tie      |\n",
       "|      8      |    0.32     |       0.65       |       0.65       |       1.00       |Tie      |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: Raw Agreement Proportions\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|\n",
       "|      1      |    0.70     |       0.85       |       0.85       |       1.00       |\n",
       "|      2      |    0.60     |       0.80       |       0.80       |       1.00       |\n",
       "|      3      |    0.60     |       0.75       |       0.75       |       0.95       |\n",
       "|      4      |    0.50     |       0.75       |       0.70       |       0.95       |\n",
       "|      5      |    0.75     |       0.82       |       0.82       |       0.95       |\n",
       "|      6      |    0.65     |       0.82       |       0.78       |       0.85       |\n",
       "|      7      |    0.40     |       0.65       |       0.65       |       1.00       |\n",
       "|      8      |    0.65     |       0.82       |       0.82       |       1.00       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: LLM Performance Comparison and Decision Rationale (Raw Agreement)\n",
       "\n",
       "| Response ID | OpenAI Raw Agreement | Claude Raw Agreement | OpenAI &#124;Bias&#124; | Claude &#124;Bias&#124; |Agreement Winner |Bias Winner |Overall Winner |\n",
       "|:-----------:|:--------------------:|:--------------------:|:-----------------------:|:-----------------------:|:----------------|:-----------|:--------------|\n",
       "|      1      |         0.85         |         0.85         |          0.15           |          0.15           |Tie              |Tie         |Tie            |\n",
       "|      2      |         0.80         |         0.80         |          0.20           |          0.20           |Tie              |Tie         |Tie            |\n",
       "|      3      |         0.75         |         0.75         |          0.20           |          0.25           |Claude           |OpenAI      |Tie            |\n",
       "|      4      |         0.75         |         0.70         |          0.10           |          0.15           |OpenAI           |OpenAI      |OpenAI         |\n",
       "|      5      |         0.82         |         0.82         |          0.02           |          0.07           |OpenAI           |OpenAI      |OpenAI         |\n",
       "|      6      |         0.82         |         0.78         |          0.18           |          0.03           |OpenAI           |Claude      |Tie            |\n",
       "|      7      |         0.65         |         0.65         |          0.35           |          0.35           |Tie              |Tie         |Tie            |\n",
       "|      8      |         0.82         |         0.82         |          0.07           |          0.07           |Tie              |Tie         |Tie            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_english %>%\n",
    "  select(ResponseId, ac1_humans, ac1_openai_vs_average_humans, \n",
    "         ac1_claude_vs_average_humans, ac1_openai_vs_claude, best_llm_overall) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\", \"Best LLM\"),\n",
    "    caption = \"AC1 Agreement Coefficients\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"l\"),\n",
    "    digits = 2\n",
    "  )\n",
    "\n",
    "  \n",
    "results_english %>%\n",
    "  select(ResponseId, raw_agreement_humans, raw_agreement_openai_vs_average_humans,\n",
    "         raw_agreement_claude_vs_average_humans, raw_agreement_openai_vs_claude) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\"),\n",
    "    caption = \"Raw Agreement Proportions\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\"),\n",
    "    digits = 2\n",
    "  )\n",
    "\n",
    "results_english %>%\n",
    "  mutate(\n",
    "    abs_bias_openai = abs(bias_openai_vs_average_humans),\n",
    "    abs_bias_claude = abs(bias_claude_vs_average_humans)\n",
    "  ) %>%\n",
    "  select(ResponseId, \n",
    "         raw_agreement_openai_vs_average_humans, raw_agreement_claude_vs_average_humans,\n",
    "         abs_bias_openai, abs_bias_claude,\n",
    "         best_llm_by_agreement, best_llm_by_bias, best_llm_overall) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \n",
    "                  \"OpenAI Raw Agreement\", \"Claude Raw Agreement\",\n",
    "                  \"OpenAI |Bias|\", \"Claude |Bias|\",\n",
    "                  \"Agreement Winner\", \"Bias Winner\", \"Overall Winner\"),\n",
    "    caption = \"LLM Performance Comparison and Decision Rationale (Raw Agreement)\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"l\", \"l\", \"l\"),\n",
    "    digits = 2\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364240a-1d22-44f6-8c26-fcf6dab2f4e5",
   "metadata": {},
   "source": [
    "## 3. Combined Analysis: Cross-Language Framework Validation\n",
    "\n",
    "### Pooled Sample Assessment\n",
    "Combining Dutch and English samples to establish overall framework reliability estimates and provide robust statistical foundation for deployment recommendations.\n",
    "\n",
    "**Statistical Advantages:**\n",
    "- Increased sample size for reliable coefficient estimation\n",
    "- Cross-language generalizability assessment\n",
    "- Comprehensive model comparison across diverse contexts\n",
    "- Framework-level performance validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf663715-9a01-4907-bd9c-32927daf246e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: AC1 Agreement Coefficients\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |Best LLM |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|:--------|\n",
       "|      1      |    0.72     |       0.87       |       0.87       |       1.00       |Tie      |\n",
       "|      2      |    0.60     |       0.82       |       0.82       |       1.00       |Tie      |\n",
       "|      3      |    0.39     |       0.69       |       0.70       |       0.97       |Tie      |\n",
       "|      4      |    0.36     |       0.54       |       0.51       |       0.97       |OpenAI   |\n",
       "|      5      |    0.65     |       0.67       |       0.66       |       0.97       |Tie      |\n",
       "|      6      |    0.53     |       0.79       |       0.69       |       0.84       |Tie      |\n",
       "|      7      |    0.33     |       0.67       |       0.67       |       1.00       |Tie      |\n",
       "|      8      |    0.55     |       0.72       |       0.72       |       0.95       |OpenAI   |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Table: Raw Agreement Proportions\n",
       "\n",
       "| Response ID | Human-Human | OpenAI vs Humans | Claude vs Humans | OpenAI vs Claude |\n",
       "|:-----------:|:-----------:|:----------------:|:----------------:|:----------------:|\n",
       "|      1      |    0.78     |       0.89       |       0.89       |       1.00       |\n",
       "|      2      |    0.70     |       0.85       |       0.85       |       1.00       |\n",
       "|      3      |    0.60     |       0.78       |       0.78       |       0.98       |\n",
       "|      4      |    0.65     |       0.72       |       0.70       |       0.98       |\n",
       "|      5      |    0.78     |       0.76       |       0.76       |       0.98       |\n",
       "|      6      |    0.68     |       0.84       |       0.79       |       0.88       |\n",
       "|      7      |    0.58     |       0.76       |       0.76       |       1.00       |\n",
       "|      8      |    0.75     |       0.85       |       0.85       |       0.98       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_combined <- bind_rows(df_dutch, df_english)\n",
    "results_combined <- calculate_ac1_metrics(df_combined, \"ResponseId\")\n",
    "\n",
    "\n",
    "results_combined %>%\n",
    "  select(ResponseId, ac1_humans, ac1_openai_vs_average_humans, \n",
    "         ac1_claude_vs_average_humans, ac1_openai_vs_claude, best_llm_overall) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\", \"Best LLM\"),\n",
    "    caption = \"AC1 Agreement Coefficients\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"l\"),\n",
    "    digits = 2\n",
    "  )\n",
    "\n",
    "  \n",
    "results_combined %>%\n",
    "  select(ResponseId, raw_agreement_humans, raw_agreement_openai_vs_average_humans,\n",
    "         raw_agreement_claude_vs_average_humans, raw_agreement_openai_vs_claude) %>%\n",
    "  knitr::kable(\n",
    "    col.names = c(\"Response ID\", \"Human-Human\", \"OpenAI vs Humans\", \n",
    "                  \"Claude vs Humans\", \"OpenAI vs Claude\"),\n",
    "    caption = \"Raw Agreement Proportions\",\n",
    "    align = c(\"c\", \"c\", \"c\", \"c\", \"c\"),\n",
    "    digits = 2\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa47c3c-5197-4687-8c4e-10a2cff89219",
   "metadata": {},
   "source": [
    "## 4. Summary and Framework Implications\n",
    "\n",
    "### LLM-as-a-Judge Validation Outcomes\n",
    "\n",
    "This analysis provides empirical validation of LLM-based evaluation reliability for the job listing quality assessment framework.\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Model Performance**: Both GPT-4o and Claude-3.5 demonstrate substantial agreement with human experts\n",
    "2. **Cross-Language Consistency**: Framework maintains reliability across Dutch and English evaluations\n",
    "3. **Automated Scalability**: LLM evaluations show comparable or superior consistency to human-only assessment\n",
    "4. **Practical Deployment**: Results support automated evaluation integration with minimal human oversight\n",
    "\n",
    "**Framework Integration:**\n",
    "- **Primary Evaluator**: GPT-4o selected based on marginal performance advantages\n",
    "- **Quality Assurance**: Dual-model validation for critical assessments\n",
    "- **Threshold Validation**: Binary classification approach confirmed for practical implementation\n",
    "- **Scalability Confirmation**: Framework ready for large-scale job listing evaluation\n",
    "\n",
    "**Next Steps:**\n",
    "These validation results enable progression to full framework implementation and real-world deployment testing in recruitment workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc51728-cb67-47f5-8e99-702d60ad73fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
